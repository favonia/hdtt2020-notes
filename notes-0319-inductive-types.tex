\documentclass[11pt]{article}
\usepackage{hdtt2020}
\usepackage{newpxmath}
\usepackage{newpxtext}

\vspace{1mm}

\newcommand{\N}{\mathbb{N}}
\newcommand{\2}{\textbf{2}}
\newcommand{\Sp}{\mathbb{S}^1}

\title{CSCI 8980 Higher-Dimensional Type Theory\\ Lecture Notes}
\author{Calvin Roth and Devon Tuma}
\date{March 27, 2020}

\begin{document}

\maketitle

\section{Wrap Up on Universes}

\begin{theorem}
The universe type $U$ is not a set. 
\end{theorem}
\begin{proof}
Consider the negation function on $\2$, which is an equivalence between $\2$ and $\2$.
This gives rise to some path \textit{neg} by univalence. 
Then if this path were identified with \textit{refl}, we would have point-wise identifications between the identity and negation functions, and in particular an identification between their images at $0$, namely an element of type $Id_2(0,1)$.
But in homework $3$ we showed that from this we can derive an element of $\bot$, so we have a contradiction.
So $U$ can't be a set.
\end{proof}

\section{Inductive Type}
We define an inductive type to be a type that is freely generated by a finite set of constructors, where intuitively we think of ``freely generated'' to mean that the elements of the type are all the objects that can be obtained by applying and composing the constructors. It also means that we don't impose any more relations on elements than we are absolutely required to. If you are familiar with freely generated groups from algebra then this should feel very similar.

\begin{example}
  $\2$ is generated by $0 : \2$ and $1 : \2$. Since neither constructor is a function type, there are no operations we can perform on these constructors. Therefore the type freely generated by these constructors has only two elements, as we would expect.
\end{example}

\begin{example}
  $A + B$ is generated by $inl : A \to A + B$ and $inr : B \to A + B$. In this case the constructors are functions, so we get some more interesting elements of the inductive type. The type freely generated by these constructors contains elements like $inl\ a$ for elements $a : A$. 
\end{example}
    
The first more interesting case of an inductive type is $\N$, which is generated by $zero : \N$ and $suc: \N \to \N$. This illustrates the fact that the type being defined can appear in both the domain and the co-domain of a constructor. The $suc$ constructor represents the construction of $n+1$ from a construction of $n$. Intuitively, the type freely generated by these constructors should contain $zero$, $suc(zero)$, $suc(suc(zero))$, and so on, since these are exactly the things we can construct from these constructors using the operations we have available. We can formalize this intuition with the following rules for the type $\N$: 

\begin{multicols}{2}
  \noindent
  \begin{equation*}
    \begin{prooftree}
      \infer0[$0$-Introduction]{\oftype{zero}{\N}}
    \end{prooftree}
  \end{equation*}
  \begin{equation*}
    \begin{prooftree}
      \hypo{\oftype{M}{\N}}
      \infer1[$suc$-Introduction]{\oftype{suc\ M}{\N}}
    \end{prooftree}
  \end{equation*}
\end{multicols}


Then we would like the elimination rule to model the classic induction principle for the natural numbers. Recall that traditionally the induction principle for the set of natural numbers states that in order to prove some proposition $P(n)$ for all $n \in \N$ it suffices to prove $P(0)$ and $P(n) \rightarrow P(n+1)$. We can model this rule with the following axiom for our inductive $\N$ type:

\begin{prooftree*}
  \hypo{x : \N : \rightarrow P : U}
  \hypo{M_0 : P[zero/x]}
  \hypo{ O : \N}
  \infer[no rule]3{y : P[x'/x] \rightarrow M_{suc} : P[(suc x' / x]}
  \infer1[$N$-Elimination]{rec_\N [x.P] (M_0 ; x.y.M_{succ} ; O) : P[O/x]}
\end{prooftree*}

With traditional induction on natural numbers, induction is a black box, we don't necessarily know what the proofs for each of the $P(n)$ we are proving look like. In type theory, we want to go one step farther now and say exactly what the proof of $P(n)$ look like for each $n$. This motivates two computation rules, that tell us which proof of $P(n)$ (or equivalently which element $p : P(n)$) is constructed by the induction principle for the different values of $O$:

\begin{prooftree*}
  \hypo{x : \N : \rightarrow P : U}
  \hypo{M_0 : P[zero/x]}
  \infer[no rule]2{y : P[x'/x] \rightarrow M_{suc} : P[(suc\ x' / x]}
  \infer1[$0$-Computation]{rec_{\N} [x.P] (M_0 ; x.y.M_{suc} ; zero) \equiv M_0 : P[zero/x]}
\end{prooftree*}

and

\begin{prooftree*}
  \hypo{x : \N : \rightarrow P : U}
  \hypo{M_0 : P[zero/x]}
  \hypo{ A : \N}
  \infer[no rule]3{y : P[x'/x] \rightarrow M_{suc} : P[(suc\ x' / x]}
  \infer1[$0$-Computation]{rec_{\N} [x.P] (M_0 ; x.y.M_{suc} ; suc\ A)) \equiv M_{suc} [\frac{A, rec_\N [x.P][M_0;x.y.M_{suc};A)}{x , y}] : P[\frac{suc\ A}{x}]}
\end{prooftree*}


The first rule says the elimination rule when applied to $zero$ is exactly the proof $M_0$ of the zero case.
The second says that the elimination rule applied to a successor $suc\ n$ is exactly the proof $M_{suc}$ applied to a recursive call to the elimination rule for $n$.

\begin{remark}
  While it was perfectly valid for us to have a constructor of type $\N \rightarrow \N$ when defining an inductive type, not all types make sense as the type of a constructor.
  For example if we want to define a type $C$, a constructor of type $(C \rightarrow 0) \rightarrow C$ is inconsistent because the corresponding recursion principle would allow us to construct an element of type $0$.
  Another example of an inconsistent type for a constructor is $((D \rightarrow Prop) \rightarrow Prop) \rightarrow D)$.
  To keep our types consistent it suffices to restrict ourselves to constructors whose domains have strict positivity.
\end{remark}


\section{Initial Algebras}
One way to formalize inductive types is to give them a universal property as initial objects in a category of appropriate ``algebras'', where the ``algebras'' in this category are types with the same structure as the structure imposed by the constructors.
This ``algebra'' structure will ensure that every object in the category is in some sense ``at most'' the inductive type we want to define.
To pin down which particular object in this category is the freely generated inductive type, we want to choose the object that has as few additional restrictions and relations as possible.
This choosing the least restricted (or most free) object will be captured by choosing the initial object.

To make these abstractions more concrete, we'll consider just the inductive type $\N$ and the corresponding category of $\N$-algebras.
We want objects in this category to be types with the same structure as $\N$.
Such a type $A$ should be types equipped with some element $zero_A : A$ and some function $suc_A : A \rightarrow A$.
We can capture this by defining the type of $\N$-algebras to be
\begin{equation*}
  \N Alg :\equiv \sum_{A : U} A \times (A \rightarrow A)
\end{equation*}
We can construct many examples of $\N-Algebras$ besides $\N$ fairly easily.
For example ($\2$, $true$, $\lambda x . \neg x$) is an $\N$-algebra.
More generally, for any inhabited type $C$, there exists some $c : C$, and  ($C$, $c$, $\lambda x . x$) is an $N$-algebra.

Next we define $\N$-homomorphism between two $\N$-algebras $(C, c_0, c_s)$ and $(D, d_0, d_s)$ to be a function $h: C \rightarrow D$ such that $h(c_0) = d_0$ and $h(c_s(c)) = d_s(h(c))$ for all $c : C$. We can pack this into a single type by defining the type of $\N$-homomorphisms to be
\begin{equation*}
  \N Hom((C,c_0,c_s), (D,d_0,d_s)) :\equiv \sum_{h : C \rightarrow D}\parens*{(h(c_0)=d_0) \times \prod_{c : C} (h(c_s(c))=d_s(h(c)))}
\end{equation*}

Then together these form a category whose objects are the $\N$-algebras and whose arrows are the $\N$-homomorphisms.
Then we claim that $\N$ is \textbf{the} initial object in this category, where an initial object is one that has a unique arrow from it to any other object in the category.
Note that while a category can have many initial objects, any two initial objects will have a unique isomorphism between them, so it makes sense to talk about \textbf{the} initial object rather than \textbf{an} initial object (This holds in any category, not just our category of $\N$-algebras).
In fact, as you might expect, any equivalence $f$ between $\N$ and another type $C$ will give rise to an isomorphic initial object in this category, namely $(C, f(0_\N), \lambda c . f(suc_\N(f^{-1}(c))))$.

We can formalize this idea of an initial object entirely within type theory, by defining initial $\N$-algebras to be those that have a unique homomorphism into any other $\N$-algebra: 
\begin{equation*}
  \text{is-initial}(I) := \Pi_{C : \N-Algebra} is-contr (\N Hom(I, C))
\end{equation*}
Note that this definition implies that any two initial $\N$ algebras have a homomorphism between them, because contractible types are inhabited.
Furthermore the composition of these maps is equivalent to the identity again by the contractibility.
Therefore these homomorphisms are equivalences, and so initial objects are equivalent, and we can talk about \textbf{the} initial $\N$-algebra.

Finally, it is possible to prove $\text{is-initial}(\N)$. As remarked in class, when we defined $\N$ we didn't imposed any additional structure aside from the exact rules of an $\N$-algebra so it shouldn't be shocking that $\text{is-initial}(\N)$ is provable.



\section{Parameters and Indexes}

For a type $T$ that depends on another type $C$, if each type in the family $T(\cdot)$ is constructed independently of each other we call the elements of $C$ type parameters of the type family $T(\cdot)$.
For example if we want to talk about the family of lists containing elements of a specific type with something like $List(A)$, then $A$ is a parameter.
We see that there is no way to apply a type constructor to an element of the type $List(\N)$ to construct an element of type $List(\2)$, nor can we do that for any distinct types in $\mathcal{U}$.

In contrast, a type index is when the types being defined by induction together, so the constructors may take an element with one index and return an element with another.
For example, consider the vector type of lists of a specified length $Vec_n(A)$ for $n : \N$ generated by the constructors $[] : Vec_0(A)$ and $cons : \prod_{n : \N} A \rightarrow Vec_n(A) \rightarrow Vec_{suc\ n}(A)$.
Note that the cons constructor mixes the value of $n$, so we say that $n$ is an index of the type family $Vec_{(\cdot)}(\cdot)$.
On the other hand, $A$ is still a type parameter for this type family, for the same reason it was a parameter of the list type.


\section{Higher Inductive Types}
Besides the inductive types defined above, we would also like to define types like the circle:
  $\Sp$ with constructors $base : \Sp$ and $loop : Id(base ; base)$.

This definition of the circle is more complicated than other types, because we've introduced a path, not just new elements.
Therefore this is a \textbf{higher inductive type}.
Writing down the rules for this type is more work than for regular inductive types, but the introduction rules are still as we'd expect:

\begin{multicols}{3}
  \noindent
  \begin{equation*}
    \begin{prooftree}
      \infer0[$\Sp$-Formation] {
        \Sp : U
      }
    \end{prooftree}
  \end{equation*}
  \begin{equation*}
    \begin{prooftree}
      \infer0[base-Intro] {
        base : \Sp
      }
    \end{prooftree}
  \end{equation*}
  \begin{equation*}
    \begin{prooftree}
      \infer0[loop-Intro] {
        loop : Id_{\Sp}(base; base)
      }
    \end{prooftree}
  \end{equation*}
\end{multicols}

In order to make sense of the uniqueness rules, we'll first have to define a dependent version of $ap$.
While $ap$ allows us to apply a function to a path to get a path in the co-domain, what we need now is a way to apply a dependent function to a path to get a path in the total space, where we think of a path in the total space as a path over the path in the base space.

After doing that we are able to give the elimination rule for this type:

\begin{equation*}
  \begin{prooftree}
    \hypo{x : \Sp \rightarrow C : U}
    \hypo{M_{base} : C[base/x]}
    \hypo{M_{loop} : base =_{loop} base}
    \hypo{N : \Sp}
    \infer4[S1-Elimination] {
      elim_{\Sp}[x.C](M_{base}; M_{loop}; N) : C[N/x]
    }
  \end{prooftree}
\end{equation*}

As well as a pair of computation rules for the two constructors:

\begin{equation*}
  \begin{prooftree}
    \hypo{x : \Sp \rightarrow C : U}
    \hypo{M_{base} : C[base/x]}
    \hypo{M_{loop} : base =_{loop} base}
    \infer3[S1-Elimination] {
      elim_{\Sp}[x.C](M_{base}; M_{loop}; base) \equiv M_{base}
    }
  \end{prooftree}
\end{equation*}
\begin{equation*}
  \begin{prooftree}
    \hypo{x : \Sp \rightarrow C : U}
    \hypo{M_{base} : C[base/x]}
    \hypo{M_{loop} : base =_{loop} base}
    \infer3[S1-Elimination] {
      apd(\lambda x . elim_{\Sp}[x.C](M_{base}; M_{loop}; N); loop) \equiv M_{loop} 
    }
  \end{prooftree}
\end{equation*}


\end{document}
